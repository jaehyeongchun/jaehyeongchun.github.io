---
title:  "[Computer Vision] Self-Supervised Learning í†ºì•„ë³´ê¸° ; iBOT"
excerpt: "ICLR 2022 DINO : iBOT ğŸ¤–: Image BERT Pre-Training with Online Tokenizer (ICLR 2022)"
comments: true
categories:
  - deeplearning
tags:
  - [Blog, Deeplearning, Self-Supervised Learning, ComputerVision, SSL, VICReg]

use_math: true
date: 2023-05-30
last_modified_at: 2023-05-30    

published: True

---

![image](https://github.com/jaehyeongchun/jaehyeongchun.github.io/assets/31461053/bb3dea3d-f756-483f-9c39-4c884f9ec323)

ICLR 2022ì—ì„œ ë°œí‘œëœ iBOT ğŸ¤–: Image BERT Pre-Training with Online Tokenizerì…ë‹ˆë‹¤.

Language model trainingì— ìˆì–´ Masked Language Modeling(MLM)ì€ ì„±ê³µì ì¸ paradigmìœ¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.

ëŒ€í‘œì ìœ¼ë¡œ BERTê°€ ê·¸ëŸ¬í–ˆì£ .

ì´ëŸ¬í•œ ì„±ê³µ ê¸°ë°˜ì—ëŠ” lingual tokenizer (ex. WordPiece, BPE, Unigram)ë¥¼ í™œìš©í•´ inputì„ semantically meaningful tokenìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í–ˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ Visual semanticsëŠ” ligual semanticsê³¼ ë‹¬ë¦¬ imageì˜ ì—°ì†ì ì¸ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ì‰½ê²Œ ë½‘ì•„ë‚´ê¸°ê°€ ì–´ë µìŠµë‹ˆë‹¤.

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ BEIT : Pre-Training of Image Transformerì—ì„œëŠ” DALL-Eì˜ pre-trained VAEë¥¼ visual tokenizerë¡œ í™œìš©í–ˆìœ¼ë‚˜
ì´ë¡œ ì¸í•´ multi-stage training pipelineì´ ë¶ˆê°€í”¼í–ˆê³  ë˜í•œ tokenizerê°€ high-level semanticsì„ ì¡ì•„ë‚´ëŠ”ë° ì–´ë ¤ì›€ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

ê·¸ë ‡ê¸°ì— ì´ ë…¼ë¬¸ì—ì„œ ì €ìëŠ” Vision transformerë¥¼ ì˜ í•™ìŠµí•˜ê¸° ìœ„í•´ Online tokenizerì™€ Knowlege distillationì„ í†µí•´
ìƒˆë¡œìš´ Masked Image Modeling (MIM) frameworkë¥¼ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. 

ì•„ë˜ì˜ ìœ íŠœë¸Œ ë™ì˜ìƒìœ¼ë¡œ ë…¼ë¬¸ì— ëŒ€í•œ ì„¤ëª…ì„ ì§„í–‰í•˜ì˜€ìœ¼ë‹ˆ í•œë²ˆ ë´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.

<iframe width="873" height="491" src="https://www.youtube.com/embed/eSlWQin30xY?list=PLyP9gclj-bv6Nh5Xp-rUMMiD8kI0kW_E8" title="[SSL] ë…¼ë¬¸ ë¦¬ë·° : iBOT ğŸ¤–: Image BERT Pre-Training with Online Tokenizer (ICLR 2022)" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>  