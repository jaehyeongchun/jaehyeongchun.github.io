---
title:  "[Computer Vision] Self-Supervised Learning 톺아보기 ; VICReg"
excerpt: "ICLR 2022 VICReg : Variance-Invariance-Covariance Regularization for Self-Supervised Learning"
comments: true
categories:
  - deeplearning
tags:
  - [Blog, Deeplearning, Self-Supervised Learning, ComputerVision, SSL, VICReg]

use_math: true
date: 2023-04-13
last_modified_at: 2022-01-19    
---
![image](https://user-images.githubusercontent.com/31461053/231626016-b2fab792-d3c7-49b0-9ab1-dd51445ab597.png){: width="70%" height="70%" .align-center}

저자인 Yann LeCun을 보면 알 수 있듯이 Facebook AI Research (FAIR)에서 작성해 ICLR 2022에 accept된 논문이다.

논문 링크는 아래에 있다.

[Bardes, Adrien, Jean Ponce, and Yann LeCun의 "VICREG: Variance-invariance-covariance regularization for self-supervised learning."](https://arxiv.org/abs/2105.04906)

이번 포스트에서는 이전의 Self-Supervised Learning(SSL)의 방법론과는 다르게 Simple하고 Explicit한 Regularization term으로 좋은 performance를 보인 VICReg에 대해 설명해보겠다.

## 요약
* Invariance term으로 image의 differen view의 invariance를 학습하고
* Variance preservation term으로 representation의 collapse를 피하고
* Covariance regularization term으로 embedding vector의 information을 최대화하면
* 복잡한 technique이나 Negative sample 없이도 SOTA 성능을 보이고 Multi-modal task에 적합한 SSL approach 완성!

## Introduction

Self-supervised learning의 main challenge는 input image와 상관없이 embedding vector가 동일한 output으로 나오는 Collapse 문제이다.

이를 해결하기 위한 방법론으로는 대표적으로 3가지가 있다.

1. Contrastive method (SimCLR,MoCo), Clustering method (DeepCluster, SwAV)와 같이 수많은 negative sample을 활용하는 방법.
2. Distillation method (BYOL, SimSiam)와 같이 teacher-student network 방식으로 Momentum encoder, Stop gradient, Predictor와 같은 techniuqe을 활용하는 방법
3. Information maximization method (Barlow Twins, W-MSE)와 같이 embedding vector를 decorrelation 시켜 information collapse 방지를 하는 방법

이번에 리뷰하게 된 VICReg은 Information maximization method에 속한다.

이전 모델들인 Whitening MSE와 Barlow Twins는 다른 Post에서 확인할 수 있다!

## Intuition
가장 기본적인 VICReg의 아이디어는 이름에서 알  수 있듯이 Variance-Invariance-Covariance이다.

이에 대한 각각의 정의를 저자는 다음과 같이 설명하고 있다.

* Variance : a hinge loss to maintain the standard deviation (over a batch) of each variable of the embedding above a given threshold. This term forces the embedding vectors of samples within a batch to be different.
* Invariance : the mean square distance between the embedding vectors.
* Covariance : a term that attracts the covariances (over a batch) between every pair of (centered) embedding variables towards zero. This term decorrelates the variables of each embedding and prevents an *informational collapse* in which the variables would vary together or be highly correlated.

뒤에서 자세히 설명하겠지만 간단히 말하면 동일한 image들은 비슷한 embedding vector value를 갖도록 (Invariance), embedding vector이 0으로 collapse 되지 않도록 (Variance), embedding vector의 variable 간의 redundancy를 줄여 informational colaapse를 막도록 (Covariance) 하는 Term들이다.

특히나 Variance와 Covariance는 각각의 branch network에 적용된다는게 중요한 특징이다. 

이를 통해서 VICReg은 이전의 방법론들에 비해
* 두개의 branch network들이 weight sharing을 하거나 동일한 network architecture를 가질 필요가 없다.
* Negative sample이 필요없기 때문에 큰 용량의 memory bank나 large batch size가 필요가 없다.
* Batch-wise normalization이나 feature-wise normalization이 필요가 없다.
* Vector quantization나 predictor module이 필요가 없다.

## Method
![image](https://user-images.githubusercontent.com/31461053/231625513-574cc038-7a26-48f0-908a-c0e6ea092ccc.png){: .align-center}

위의 figure가 VICReg의 전체적인 구조이다. joint embedding architecture로 input image가 서로 다른 transformation을 거쳐 encoder, 
expander를 거쳐 embedding vector를 뽑아내고 이에 대해 variance (v), covariance (c), invariance (s) term을 산출한다.

물론 각 branch network의 architecture는 동일할 필요가 없으나 이 논문을 위한 대부분의 실험에서는 동일한 network, weight sharing을 적용했다.
Encoder는 ResNet-50 backbone을 사용하였고 output dimension은 2048, expander는 size 8192의 3 fully-connected layer를 사용했다.

이제 이 논문의 핵심이라고 볼 수 있는 각각의 term에 대해 자세히 살펴보겠다.

**1. Variance Preservation term**
![image](https://user-images.githubusercontent.com/31461053/231627980-5e924d02-ad14-40ef-b344-af7cb79aaea3.png){: width="70%" height="70%" .align-center}

우선 expander까지 통과한 embedding vector Z와 Z'은 N by d dimension을 가진다. N은 input image의 batch-size, d는 embedding vector의 dimension size이다. 

Variance Preservation term은 embedding vector의 각 dimension이 특정 값 이상의 variance를 강제로 가지도록 하여 모든 iput에 대해
동일한 embedding vector를 만들어내는 collapse를 방지하기 위한 term이다. 위의 그림처럼 검은 점선 네모로 표현된 부분의 dimension 값들이
동일한 값이 아닌 variance를 일정 수준 이상으로 유지하는 것이 목적이다.

즉, Embedding vector의 N개의 j번째 dimension 값들의 standard deviation이 $\gamma$ 이상이 되도록 학습시켜 모든 input이 같은 
embedding vector로 mapping되는 collapse를 방지하는 것이 목적인 term이다. 때문에 이는 SVM에서 사용되는 loss인 hinge loss 형태로 다음과 같이 표현된다.

<center>
$v(z)=\frac{1}{d}\displaystyle\sum_{j=1}^{d} max(0, \gamma - S(z^j, \epsilon)) \quad where \; S(x, \epsilon) = \sqrt{Var(x)+\epsilon}$
</center>


여기서 $\epsilon$은 학습과정에서 numerical stability를 위해 더해주는 작은 scalar value (0.0001)이고 $\gamma$는 1로 설정했다.

이 때 variance 대신 standard deviation 값을 사용한 이유는 $Var(x)$의 gradient가 정의상 $x$가 $x$의 평균인 $\overline{x}$에 가까워지는 경우
gradient가 0에 가까워지고 이는 결국 학습이 되지 않아 collapse가 일어날 가능성이 커지기 때문이다.
 
여기서 주목해야할 점은 이 variance term은 각각의 branch에서 따로 구해진다는 것이다. 


**2. Covariance Regularization term**
![image](https://user-images.githubusercontent.com/31461053/231664862-2c387ca9-bf9a-40da-9820-ec1c79fb15c7.png){: width="70%" height="70%" .align-center}

다음은 Covariance Regularization term이다.

저자도 이 부분은 Barlow twins에게서 영감을 받았다고 한다.

중요한 차이점은 Barlow twins에서 구한 것은 서로 다른 branch에서 나온 embedding인 Z와 Z'의 cross-correlation을 구했다면,
VICReg의 Covariance Regularization term에서는 각각의 branch에서 나온 embedding 내에서 covariance를 구한다는 점이다.

위의 그림에서 보다시피 위쪽 branch에서 나온 embedding vector Z에서 빨간색 점선 네모로 표시한 것처럼 dimension 끼리의 covariance를 구하고
아래쪽도 동일한 방식으로 covariance를 구한다. 그렇게 각 dimension 사이의 covariance를 담고 있는 Covariance Matrix를 구한다.
이 때 Variance preservation term과 같이 branch마다 각각 계산된다는 점을 잘 기억하자.

Covariance matrix를 의미하는 수식을 살펴보면 다음과 같다.

<center>
$C(Z)=\frac{1}{n-1}\displaystyle\sum_{i=1}^{n}(z_i-\overline{z})(z_i-\overline{z})^T \quad where \; \overline{z}=\frac{1}{n}\displaystyle\sum_{i=1}^{n}z_i$
</center>

Covariance matrix에서 우리에게 중요한 것은 off-diagnoal coefficient이다. Diagonal coefficient는 당연하게도 자기 자신과의 covariance 이므로
1이 될 것이고 그 외의 다른 dimension 사이의 covariance는 0에 가까워질수록 서로 관련이 없는 정보를 encoding한다고 할 수 있다. 그렇기 때문에 
$C(z)$의 off-diagonal coefficient들만 모은 것을 covriance regularization term이라고 하고 다음과 같이 나타낸다.

<center>
$c(Z)=\frac{1}{d}\displaystyle\sum_{i\neq j}[C(Z)]^2_{i,j}$
</center>

이 $c(Z)$를 작게 만들수록 embedding vector의 dimensoin 사이의 decorrelation이 커질 것이고 이는 유사한 정보를 encoding하게 되는
informational collapse를 방지할 수 있게 될 것이다.

**3. Invariance term**
![image](https://user-images.githubusercontent.com/31461053/231674623-7aebf9ab-8cb2-4446-ab77-7bc1ef85cd21.png){: width="70%" height="70%" .align-center}

마지막으로 Invariance term이다. 이는 Self-supervised Learning에서는 굉장히 익숙한 개념이다.

위의 그림과 같이 동일한 image로부터 나온 sample들의 embedding vector 사이의 거리를 의미하는 term이며 이를 수식으로 표현하면 아래와 같다.

<center>
$s(Z,Z')=\frac{1}{n}\displaystyle\sum_{i}\lVert z_i-z_i^{'} \rVert_2^2$
</center>

Embedding vector $Z와 Z'$ 사이의 mean-squared euclidean distance로 표현되고 있다.

만일 이 term만 loss로 사용하여 학습한다면 모든 embedding vector가 동일해져 우리가 우려했던 collapse가 일어날 것이다. 
그렇기 때문에 해당 논문에서 추가적인 variance와 covariance term을 반영한 것이다.

---

그럼 지금까지 살펴본 term들을 종합해 loss function $l$을 구성해보자.

<center>
$l(Z,Z') = \lambda s(Z,Z') + \mu [v(Z)+v(Z')] + \nu [c(Z)+c(Z')]$
</center>

수식을 보면 알 수 있듯이 invariance term은 두 개의 branch network로 부터 하나의 term만 나오고 variance term과 covariance term은 각각의 embedding vector Z와 Z' 각각에서 나오기 때문에 이 둘의 합으로 이루어져있다.

여기서 $\lambda, \mu, \nu$는 hyper-parameter로 $\nu=1$로 setting하고 나머지 $\lambda$와 $\mu$는 같고 1보다 크다는 조건하에 grid search를 진행했다.
그 결과 $\lambda$와 $\mu$는 25일 때 가장 좋은 성능을 보였다.

그리고 이 loss function을 unlabelled dataset $D$의 모든 이미지에 대해 구하면

<center>
$L=\displaystyle\sum_{I\subset D}
</center>



 
